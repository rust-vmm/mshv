// Copyright Â© 2020, Microsoft Corporation
//
// SPDX-License-Identifier: Apache-2.0 OR BSD-3-Clause
//

use crate::bindings::*;
use crate::hvdef::HV_X64_MSR_GUEST_OS_ID;
#[cfg(feature = "with-serde")]
use serde_derive::{Deserialize, Serialize};
use std::cmp;
use std::fmt;
use std::ptr;
use vmm_sys_util::errno;
use zerocopy::{AsBytes, FromBytes};

#[repr(C)]
#[derive(Default)]
pub struct __IncompleteArrayField<T>(::std::marker::PhantomData<T>, [T; 0]);
impl<T> __IncompleteArrayField<T> {
    #[inline]
    pub fn new() -> Self {
        __IncompleteArrayField(::std::marker::PhantomData, [])
    }

    #[inline]
    /// # Safety
    /// Safe Beacuse we know the size of the field.
    /// Caller needs to make sure lossless conversion
    pub unsafe fn as_ptr(&self) -> *const T {
        self as *const __IncompleteArrayField<T> as *const T
    }
    #[inline]
    /// # Safety
    /// Safe Beacuse we know the size of the field.
    /// Caller needs to make sure lossless conversion
    pub unsafe fn as_mut_ptr(&mut self) -> *mut T {
        self as *const __IncompleteArrayField<T> as *mut T
    }
    #[inline]
    /// # Safety
    /// Safe Beacuse we know the size of the field.
    /// Caller needs to make sure lossless conversion
    pub unsafe fn as_slice(&self, len: usize) -> &[T] {
        ::std::slice::from_raw_parts(self.as_ptr(), len)
    }
    #[inline]
    /// # Safety
    /// Safe Beacuse we know the size of the field.
    /// Caller needs to make sure lossless conversion
    pub unsafe fn as_mut_slice(&mut self, len: usize) -> &mut [T] {
        ::std::slice::from_raw_parts_mut(self.as_mut_ptr(), len)
    }
}
impl<T> ::std::fmt::Debug for __IncompleteArrayField<T> {
    fn fmt(&self, fmt: &mut ::std::fmt::Formatter<'_>) -> ::std::fmt::Result {
        fmt.write_str("__IncompleteArrayField")
    }
}
impl<T> ::std::clone::Clone for __IncompleteArrayField<T> {
    #[inline]
    fn clone(&self) -> Self {
        Self::new()
    }
}

#[repr(C)]
#[derive(Debug, Default, Copy, Clone, Eq, PartialEq, AsBytes, FromBytes)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct StandardRegisters {
    pub rax: u64,
    pub rbx: u64,
    pub rcx: u64,
    pub rdx: u64,
    pub rsi: u64,
    pub rdi: u64,
    pub rsp: u64,
    pub rbp: u64,
    pub r8: u64,
    pub r9: u64,
    pub r10: u64,
    pub r11: u64,
    pub r12: u64,
    pub r13: u64,
    pub r14: u64,
    pub r15: u64,
    pub rip: u64,
    pub rflags: u64,
}

#[repr(C)]
#[derive(Debug, Default, Copy, Clone, AsBytes, FromBytes)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct SegmentRegister {
    /* segment register + descriptor */
    pub base: u64,
    pub limit: u32,
    pub selector: u16,
    pub type_: u8,   /* type, writeable etc: 4 */
    pub present: u8, /* if not present, exception generated: 1 */
    pub dpl: u8,     /* descriptor privilege level (ring): 2 */
    pub db: u8,      /* default/big (16 or 32 bit size offset): 1 */
    pub s: u8,       /* non-system segment */
    pub l: u8,       /* long (64 bit): 1 */
    pub g: u8,       /* granularity (bytes or 4096 byte pages): 1 */
    pub avl: u8,     /* available (free bit for software to use): 1 */
    pub unusable: __u8,
    pub padding: __u8,
}

impl From<hv_x64_segment_register> for SegmentRegister {
    fn from(hv_reg: hv_x64_segment_register) -> Self {
        let mut reg = SegmentRegister {
            base: hv_reg.base,
            limit: hv_reg.limit,
            selector: hv_reg.selector,
            unusable: 0_u8,
            padding: 0_u8,
            ..Default::default()
        };

        // SAFETY: Getting a bunch of bitfields. Functions and unions are generated by bindgen
        // so we have to use unsafe here. We trust bindgen to generate the correct accessors.
        unsafe {
            reg.type_ = hv_reg.__bindgen_anon_1.__bindgen_anon_1.segment_type() as u8;
            reg.present = hv_reg.__bindgen_anon_1.__bindgen_anon_1.present() as u8;
            reg.dpl = hv_reg
                .__bindgen_anon_1
                .__bindgen_anon_1
                .descriptor_privilege_level() as u8;
            reg.db = hv_reg.__bindgen_anon_1.__bindgen_anon_1._default() as u8;
            reg.s = hv_reg
                .__bindgen_anon_1
                .__bindgen_anon_1
                .non_system_segment() as u8;
            reg.l = hv_reg.__bindgen_anon_1.__bindgen_anon_1._long() as u8;
            reg.g = hv_reg.__bindgen_anon_1.__bindgen_anon_1.granularity() as u8;
            reg.avl = hv_reg.__bindgen_anon_1.__bindgen_anon_1.available() as u8;
        }

        reg
    }
}
impl From<SegmentRegister> for hv_x64_segment_register {
    fn from(reg: SegmentRegister) -> Self {
        let mut hv_reg = hv_x64_segment_register {
            base: reg.base,
            limit: reg.limit,
            selector: reg.selector,
            ..Default::default()
        };

        // SAFETY: Setting a bunch of bitfields. Functions and unions are generated by bindgen
        // so we have to use unsafe here. We trust bindgen to generate the correct accessors.
        unsafe {
            hv_reg
                .__bindgen_anon_1
                .__bindgen_anon_1
                .set_segment_type(reg.type_ as u16);
            hv_reg
                .__bindgen_anon_1
                .__bindgen_anon_1
                .set_present(reg.present as u16);
            hv_reg
                .__bindgen_anon_1
                .__bindgen_anon_1
                .set_descriptor_privilege_level(reg.dpl as u16);
            hv_reg
                .__bindgen_anon_1
                .__bindgen_anon_1
                .set__default(reg.db as u16);
            hv_reg
                .__bindgen_anon_1
                .__bindgen_anon_1
                .set_non_system_segment(reg.s as u16);
            hv_reg
                .__bindgen_anon_1
                .__bindgen_anon_1
                .set__long(reg.l as u16);
            hv_reg
                .__bindgen_anon_1
                .__bindgen_anon_1
                .set_granularity(reg.g as u16);
            hv_reg
                .__bindgen_anon_1
                .__bindgen_anon_1
                .set_available(reg.avl as u16);
        }

        hv_reg
    }
}

#[repr(C)]
#[derive(Debug, Default, Copy, Clone, FromBytes)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct TableRegister {
    pub base: u64,
    pub limit: u16,
}

impl From<hv_x64_table_register> for TableRegister {
    fn from(reg: hv_x64_table_register) -> Self {
        TableRegister {
            base: reg.base,
            limit: reg.limit,
        }
    }
}

impl From<TableRegister> for hv_x64_table_register {
    fn from(reg: TableRegister) -> Self {
        hv_x64_table_register {
            limit: reg.limit,
            base: reg.base,
            pad: [0; 3],
        }
    }
}

#[repr(C)]
#[derive(Debug, Default, Copy, Clone, FromBytes)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct SpecialRegisters {
    pub cs: SegmentRegister,
    pub ds: SegmentRegister,
    pub es: SegmentRegister,
    pub fs: SegmentRegister,
    pub gs: SegmentRegister,
    pub ss: SegmentRegister,
    pub tr: SegmentRegister,
    pub ldt: SegmentRegister,
    pub gdt: TableRegister,
    pub idt: TableRegister,
    pub cr0: u64,
    pub cr2: u64,
    pub cr3: u64,
    pub cr4: u64,
    pub cr8: u64,
    pub efer: u64,
    pub apic_base: u64,
    pub interrupt_bitmap: [u64; 4usize],
}

#[repr(C)]
#[derive(Debug, Default, Copy, Clone, Eq, PartialEq, AsBytes, FromBytes)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct DebugRegisters {
    pub dr0: u64,
    pub dr1: u64,
    pub dr2: u64,
    pub dr3: u64,
    pub dr6: u64,
    pub dr7: u64,
}

#[repr(C)]
#[derive(Debug, Default, Copy, Clone, Eq, PartialEq, AsBytes, FromBytes)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct FloatingPointUnit {
    pub fpr: [[u8; 16usize]; 8usize],
    pub fcw: u16,
    pub fsw: u16,
    pub ftwx: u8,
    pub pad1: u8,
    pub last_opcode: u16,
    pub last_ip: u64,
    pub last_dp: u64,
    pub xmm: [[u8; 16usize]; 16usize],
    pub mxcsr: u32,
    pub pad2: u32,
}

pub const IA32_MSR_TSC: u32 = 0x00000010;
pub const IA32_MSR_EFER: u32 = 0xC0000080;
pub const IA32_MSR_KERNEL_GS_BASE: u32 = 0xC0000102;
pub const IA32_MSR_APIC_BASE: u32 = 0x0000001B;
pub const IA32_MSR_PAT: u32 = 0x0277;
pub const IA32_MSR_SYSENTER_CS: u32 = 0x00000174;
pub const IA32_MSR_SYSENTER_ESP: u32 = 0x00000175;
pub const IA32_MSR_SYSENTER_EIP: u32 = 0x00000176;
pub const IA32_MSR_STAR: u32 = 0xC0000081;
pub const IA32_MSR_LSTAR: u32 = 0xC0000082;
pub const IA32_MSR_CSTAR: u32 = 0xC0000083;
pub const IA32_MSR_SFMASK: u32 = 0xC0000084;

pub const IA32_MSR_MTRR_CAP: u32 = 0x00FE;
pub const IA32_MSR_MTRR_DEF_TYPE: u32 = 0x02FF;
pub const IA32_MSR_MTRR_PHYSBASE0: u32 = 0x0200;
pub const IA32_MSR_MTRR_PHYSMASK0: u32 = 0x0201;
pub const IA32_MSR_MTRR_PHYSBASE1: u32 = 0x0202;
pub const IA32_MSR_MTRR_PHYSMASK1: u32 = 0x0203;
pub const IA32_MSR_MTRR_PHYSBASE2: u32 = 0x0204;
pub const IA32_MSR_MTRR_PHYSMASK2: u32 = 0x0205;
pub const IA32_MSR_MTRR_PHYSBASE3: u32 = 0x0206;
pub const IA32_MSR_MTRR_PHYSMASK3: u32 = 0x0207;
pub const IA32_MSR_MTRR_PHYSBASE4: u32 = 0x0208;
pub const IA32_MSR_MTRR_PHYSMASK4: u32 = 0x0209;
pub const IA32_MSR_MTRR_PHYSBASE5: u32 = 0x020A;
pub const IA32_MSR_MTRR_PHYSMASK5: u32 = 0x020B;
pub const IA32_MSR_MTRR_PHYSBASE6: u32 = 0x020C;
pub const IA32_MSR_MTRR_PHYSMASK6: u32 = 0x020D;
pub const IA32_MSR_MTRR_PHYSBASE7: u32 = 0x020E;
pub const IA32_MSR_MTRR_PHYSMASK7: u32 = 0x020F;

pub const IA32_MSR_MTRR_FIX64K_00000: u32 = 0x0250;
pub const IA32_MSR_MTRR_FIX16K_80000: u32 = 0x0258;
pub const IA32_MSR_MTRR_FIX16K_A0000: u32 = 0x0259;
pub const IA32_MSR_MTRR_FIX4K_C0000: u32 = 0x0268;
pub const IA32_MSR_MTRR_FIX4K_C8000: u32 = 0x0269;
pub const IA32_MSR_MTRR_FIX4K_D0000: u32 = 0x026A;
pub const IA32_MSR_MTRR_FIX4K_D8000: u32 = 0x026B;
pub const IA32_MSR_MTRR_FIX4K_E0000: u32 = 0x026C;
pub const IA32_MSR_MTRR_FIX4K_E8000: u32 = 0x026D;
pub const IA32_MSR_MTRR_FIX4K_F0000: u32 = 0x026E;
pub const IA32_MSR_MTRR_FIX4K_F8000: u32 = 0x026F;

pub const IA32_MSR_TSC_AUX: u32 = 0xC0000103;
pub const IA32_MSR_BNDCFGS: u32 = 0x00000d90;
pub const IA32_MSR_DEBUG_CTL: u32 = 0x1D9;
pub const IA32_MSR_SPEC_CTRL: u32 = 0x00000048;
pub const IA32_MSR_TSC_ADJUST: u32 = 0x0000003b;

pub const IA32_MSR_MISC_ENABLE: u32 = 0x000001a0;

pub fn msr_to_hv_reg_name(msr: u32) -> Result<::std::os::raw::c_uint, &'static str> {
    match msr {
        IA32_MSR_TSC => Ok(hv_x64_register_name_HV_X64_REGISTER_TSC),

        IA32_MSR_EFER => Ok(hv_x64_register_name_HV_X64_REGISTER_EFER),
        IA32_MSR_KERNEL_GS_BASE => Ok(hv_x64_register_name_HV_X64_REGISTER_KERNEL_GS_BASE),
        IA32_MSR_APIC_BASE => Ok(hv_x64_register_name_HV_X64_REGISTER_APIC_BASE),
        IA32_MSR_PAT => Ok(hv_x64_register_name_HV_X64_REGISTER_PAT),
        IA32_MSR_SYSENTER_CS => Ok(hv_x64_register_name_HV_X64_REGISTER_SYSENTER_CS),
        IA32_MSR_SYSENTER_ESP => Ok(hv_x64_register_name_HV_X64_REGISTER_SYSENTER_ESP),
        IA32_MSR_SYSENTER_EIP => Ok(hv_x64_register_name_HV_X64_REGISTER_SYSENTER_EIP),
        IA32_MSR_STAR => Ok(hv_x64_register_name_HV_X64_REGISTER_STAR),
        IA32_MSR_LSTAR => Ok(hv_x64_register_name_HV_X64_REGISTER_LSTAR),
        IA32_MSR_CSTAR => Ok(hv_x64_register_name_HV_X64_REGISTER_CSTAR),
        IA32_MSR_SFMASK => Ok(hv_x64_register_name_HV_X64_REGISTER_SFMASK),

        IA32_MSR_MTRR_CAP => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_CAP),
        IA32_MSR_MTRR_DEF_TYPE => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_DEF_TYPE),
        IA32_MSR_MTRR_PHYSBASE0 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_BASE0),
        IA32_MSR_MTRR_PHYSMASK0 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_MASK0),
        IA32_MSR_MTRR_PHYSBASE1 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_BASE1),
        IA32_MSR_MTRR_PHYSMASK1 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_MASK1),
        IA32_MSR_MTRR_PHYSBASE2 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_BASE2),
        IA32_MSR_MTRR_PHYSMASK2 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_MASK2),
        IA32_MSR_MTRR_PHYSBASE3 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_BASE3),
        IA32_MSR_MTRR_PHYSMASK3 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_MASK3),
        IA32_MSR_MTRR_PHYSBASE4 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_BASE4),
        IA32_MSR_MTRR_PHYSMASK4 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_MASK4),
        IA32_MSR_MTRR_PHYSBASE5 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_BASE5),
        IA32_MSR_MTRR_PHYSMASK5 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_MASK5),
        IA32_MSR_MTRR_PHYSBASE6 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_BASE6),
        IA32_MSR_MTRR_PHYSMASK6 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_MASK6),
        IA32_MSR_MTRR_PHYSBASE7 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_BASE7),
        IA32_MSR_MTRR_PHYSMASK7 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_PHYS_MASK7),

        IA32_MSR_MTRR_FIX64K_00000 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_FIX64K00000),
        IA32_MSR_MTRR_FIX16K_80000 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_FIX16K80000),
        IA32_MSR_MTRR_FIX16K_A0000 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_FIX16KA0000),
        IA32_MSR_MTRR_FIX4K_C0000 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_FIX4KC0000),
        IA32_MSR_MTRR_FIX4K_C8000 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_FIX4KC8000),
        IA32_MSR_MTRR_FIX4K_D0000 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_FIX4KD0000),
        IA32_MSR_MTRR_FIX4K_D8000 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_FIX4KD8000),
        IA32_MSR_MTRR_FIX4K_E0000 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_FIX4KE0000),
        IA32_MSR_MTRR_FIX4K_E8000 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_FIX4KE8000),
        IA32_MSR_MTRR_FIX4K_F0000 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_FIX4KF0000),
        IA32_MSR_MTRR_FIX4K_F8000 => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_MTRR_FIX4KF8000),

        IA32_MSR_TSC_AUX => Ok(hv_x64_register_name_HV_X64_REGISTER_TSC_AUX),
        IA32_MSR_BNDCFGS => Ok(hv_x64_register_name_HV_X64_REGISTER_BNDCFGS),
        IA32_MSR_DEBUG_CTL => Ok(hv_x64_register_name_HV_X64_REGISTER_DEBUG_CTL),
        IA32_MSR_TSC_ADJUST => Ok(hv_x64_register_name_HV_X64_REGISTER_TSC_ADJUST),
        IA32_MSR_SPEC_CTRL => Ok(hv_x64_register_name_HV_X64_REGISTER_SPEC_CTRL),
        HV_X64_MSR_GUEST_OS_ID => Ok(hv_register_name_HV_REGISTER_GUEST_OS_ID),

        IA32_MSR_MISC_ENABLE => Ok(hv_x64_register_name_HV_X64_REGISTER_MSR_IA32_MISC_ENABLE),
        _ => Err("Not a supported hv_register_name msr"),
    }
}

#[repr(C)]
#[derive(Debug, Default, Copy, Clone, Eq, PartialEq, AsBytes, FromBytes)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct msr_entry {
    pub index: u32,
    pub reserved: u32,
    pub data: u64,
}

#[repr(C)]
#[derive(Debug, Default)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct msrs {
    pub nmsrs: u32,
    #[cfg_attr(feature = "with-serde", serde(skip))]
    pub pad: u32,
    #[cfg_attr(feature = "with-serde", serde(skip))]
    pub entries: __IncompleteArrayField<msr_entry>,
}

#[repr(C)]
#[derive(Debug, Default)]
pub struct msr_list {
    pub nmsrs: u32,
    pub indices: __IncompleteArrayField<u32>,
}

#[repr(C)]
#[derive(Debug, Default, Copy, Clone, Eq, PartialEq, AsBytes, FromBytes)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct VcpuEvents {
    pub pending_interruption: u64,
    pub interrupt_state: u64,
    pub internal_activity_state: u64,
    pub pending_event0: [u8; 16usize],
    pub pending_event1: [u8; 16usize],
}
#[repr(C)]
#[derive(Debug, Default, Copy, Clone, Eq, PartialEq, AsBytes, FromBytes)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct Xcrs {
    pub xcr0: u64,
}

#[repr(C)]
#[derive(Debug, Default, Copy, Clone, Eq, PartialEq, AsBytes, FromBytes)]
pub struct hv_cpuid_entry {
    pub function: __u32,
    pub index: __u32,
    pub flags: __u32,
    pub eax: __u32,
    pub ebx: __u32,
    pub ecx: __u32,
    pub edx: __u32,
    pub padding: [__u32; 3usize],
}
#[repr(C)]
#[derive(Debug, Default)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct hv_cpuid {
    pub nent: __u32,
    #[cfg_attr(feature = "with-serde", serde(skip))]
    pub padding: __u32,
    #[cfg_attr(feature = "with-serde", serde(skip))]
    pub entries: __IncompleteArrayField<hv_cpuid_entry>,
}

pub const LOCAL_APIC_OFFSET_APIC_ID: isize = 0x20; // APIC ID Register.
pub const LOCAL_APIC_OFFSET_VERSION: isize = 0x30; // APIC Version Register.
pub const LOCAL_APIC_OFFSET_TPR: isize = 0x80; // Task Priority Register
pub const LOCAL_APIC_OFFSET_APR: isize = 0x90; // Arbitration Priority Register.
pub const LOCAL_APIC_OFFSET_PPR: isize = 0xA0; // Processor Priority Register.
pub const LOCAL_APIC_OFFSET_EOI: isize = 0xB0; // End Of Interrupt Register.
pub const LOCAL_APIC_OFFSET_REMOTE_READ: isize = 0xC0; // Remote Read Register
pub const LOCAL_APIC_OFFSET_LDR: isize = 0xD0; // Logical Destination Register.
pub const LOCAL_APIC_OFFSET_DFR: isize = 0xE0; // Destination Format Register.
pub const LOCAL_APIC_OFFSET_SPURIOUS: isize = 0xF0; // Spurious Interrupt Vector.
pub const LOCAL_APIC_OFFSET_ISR: isize = 0x100; // In-Service Register.
pub const LOCAL_APIC_OFFSET_TMR: isize = 0x180; // Trigger Mode Register.
pub const LOCAL_APIC_OFFSET_IRR: isize = 0x200; // Interrupt Request Register.
pub const LOCAL_APIC_OFFSET_ERROR: isize = 0x280; // Error Status Register.
pub const LOCAL_APIC_OFFSET_ICR_LOW: isize = 0x300; // ICR Low.
pub const LOCAL_APIC_OFFSET_ICR_HIGH: isize = 0x310; // ICR High.
pub const LOCAL_APIC_OFFSET_TIMER_LVT: isize = 0x320; // LVT Timer Register.
pub const LOCAL_APIC_OFFSET_THERMAL_LVT: isize = 0x330; // LVT Thermal Register.
pub const LOCAL_APIC_OFFSET_PERFMON_LVT: isize = 0x340; // LVT Performance Monitor Register.
pub const LOCAL_APIC_OFFSET_LINT0_LVT: isize = 0x350; // LVT Local Int0; Register.
pub const LOCAL_APIC_OFFSET_LINT1_LVT: isize = 0x360; // LVT Local Int1 Register.
pub const LOCAL_APIC_OFFSET_ERROR_LVT: isize = 0x370; // LVT Error Register.
pub const LOCAL_APIC_OFFSET_INITIAL_COUNT: isize = 0x380; // Initial count Register.
pub const LOCAL_APIC_OFFSET_CURRENT_COUNT: isize = 0x390; // R/O Current count Register.
pub const LOCAL_APIC_OFFSET_DIVIDER: isize = 0x3e0; // Divide configuration Register.
pub const LOCAL_X2APIC_OFFSET_SELF_IPI: isize = 0x3f0; // Self IPI register, only present in x2APIC.

pub struct Buffer {
    pub layout: std::alloc::Layout,
    pub buf: *mut u8,
}

impl Buffer {
    pub fn new(size: usize, align: usize) -> Result<Buffer, errno::Error> {
        let layout = std::alloc::Layout::from_size_align(size, align).unwrap();
        // SAFETY: layout is valid
        let buf = unsafe { std::alloc::alloc(layout) };
        if buf.is_null() {
            return Err(errno::Error::new(libc::ENOMEM));
        }

        let buf = Buffer { layout, buf };

        Ok(buf)
    }

    pub fn dealloc(self) {
        // SAFETY: buf was allocated with layout
        unsafe {
            std::alloc::dealloc(self.buf, self.layout);
        }
    }

    pub fn size(&self) -> usize {
        self.layout.size()
    }
}

impl Drop for Buffer {
    fn drop(&mut self) {
        // SAFETY: buf was allocated with layout
        unsafe {
            std::alloc::dealloc(self.buf, self.layout);
        }
    }
}

#[repr(C)]
#[derive(Copy, Clone, Debug, AsBytes, FromBytes)]
pub struct LapicState {
    pub regs: [::std::os::raw::c_char; 1024usize],
}
impl Default for LapicState {
    fn default() -> Self {
        unsafe { ::std::mem::zeroed() }
    }
}
/*
impl Default for hv_register_value {
    fn default() -> Self {
        unsafe { ::std::mem::zeroed() }
    }
} */
#[repr(C)]
#[derive(Copy, Clone, Debug, AsBytes, FromBytes)]
/// This struct normalizes the actual mhsv XSave structure
/// XSave only used in save and restore functionalities, serilization and
/// deserialization are needed. Putting all the fields into a single buffer makes
/// it easier to serialize and deserialize
/// Total buffer size: 4120
/// flags: 8 bytes
/// states: 8 bytes
/// data_size: 8 bytes
/// Actual xsave buffer: 4096 bytes
pub struct XSave {
    pub buffer: [::std::os::raw::c_char; 4120usize],
}

impl Default for XSave {
    fn default() -> Self {
        unsafe { ::std::mem::zeroed() }
    }
}

impl From<mshv_vp_state> for XSave {
    fn from(reg: mshv_vp_state) -> Self {
        let ret = XSave {
            ..Default::default()
        };
        let mut bs = reg.xsave.flags.to_le_bytes();
        unsafe {
            ptr::copy(
                bs.as_ptr() as *mut u8,
                ret.buffer.as_ptr().offset(0) as *mut u8,
                8,
            )
        };
        bs = unsafe { reg.xsave.states.as_uint64.to_le_bytes() };
        unsafe {
            ptr::copy(
                bs.as_ptr() as *mut u8,
                ret.buffer.as_ptr().offset(8) as *mut u8,
                8,
            )
        };
        bs = reg.buf_size.to_le_bytes();
        unsafe {
            ptr::copy(
                bs.as_ptr() as *mut u8,
                ret.buffer.as_ptr().offset(16) as *mut u8,
                8,
            )
        };
        let min: usize = cmp::min(4096, reg.buf_size as u32) as usize;
        unsafe {
            ptr::copy(
                reg.buf.bytes,
                ret.buffer.as_ptr().offset(24) as *mut u8,
                min,
            )
        };
        ret
    }
}

impl From<XSave> for mshv_vp_state {
    fn from(reg: XSave) -> Self {
        let flags = reg.flags();
        let states = reg.states();
        let buffer_size = reg.data_size();
        let mut ret = mshv_vp_state {
            type_: hv_get_set_vp_state_type_HV_GET_SET_VP_STATE_XSAVE,
            buf_size: buffer_size,
            ..Default::default()
        };
        ret.xsave.flags = flags;
        ret.xsave.states.as_uint64 = states;
        ret.buf.bytes = reg.data_buffer() as *mut u8;
        ret
    }
}
impl From<mshv_vp_state> for LapicState {
    fn from(reg: mshv_vp_state) -> Self {
        let mut ret: LapicState = LapicState::default();
        let state = ret.regs.as_mut_ptr();
        let hv_state = unsafe { *reg.buf.lapic };
        unsafe {
            *(state.offset(LOCAL_APIC_OFFSET_APIC_ID) as *mut u32) = hv_state.apic_id;
            *(state.offset(LOCAL_APIC_OFFSET_VERSION) as *mut u32) = hv_state.apic_version;
            *(state.offset(LOCAL_APIC_OFFSET_REMOTE_READ) as *mut u32) = hv_state.apic_remote_read;
            *(state.offset(LOCAL_APIC_OFFSET_LDR) as *mut u32) = hv_state.apic_ldr;
            *(state.offset(LOCAL_APIC_OFFSET_DFR) as *mut u32) = hv_state.apic_dfr;
            *(state.offset(LOCAL_APIC_OFFSET_SPURIOUS) as *mut u32) = hv_state.apic_spurious;
            *(state.offset(LOCAL_APIC_OFFSET_ERROR) as *mut u32) = hv_state.apic_esr;
            *(state.offset(LOCAL_APIC_OFFSET_ICR_LOW) as *mut u32) = hv_state.apic_icr_low;
            *(state.offset(LOCAL_APIC_OFFSET_ICR_HIGH) as *mut u32) = hv_state.apic_icr_high;
            *(state.offset(LOCAL_APIC_OFFSET_TIMER_LVT) as *mut u32) = hv_state.apic_lvt_timer;
            *(state.offset(LOCAL_APIC_OFFSET_THERMAL_LVT) as *mut u32) = hv_state.apic_lvt_thermal;
            *(state.offset(LOCAL_APIC_OFFSET_PERFMON_LVT) as *mut u32) = hv_state.apic_lvt_perfmon;
            *(state.offset(LOCAL_APIC_OFFSET_LINT0_LVT) as *mut u32) = hv_state.apic_lvt_lint0;
            *(state.offset(LOCAL_APIC_OFFSET_LINT1_LVT) as *mut u32) = hv_state.apic_lvt_lint1;
            *(state.offset(LOCAL_APIC_OFFSET_ERROR_LVT) as *mut u32) = hv_state.apic_lvt_error;
            *(state.offset(LOCAL_APIC_OFFSET_INITIAL_COUNT) as *mut u32) =
                hv_state.apic_initial_count;
            *(state.offset(LOCAL_APIC_OFFSET_CURRENT_COUNT) as *mut u32) =
                hv_state.apic_counter_value;
            *(state.offset(LOCAL_APIC_OFFSET_DIVIDER) as *mut u32) =
                hv_state.apic_divide_configuration;
        }

        /* vectors ISR TMR IRR */
        for i in 0..8 {
            unsafe {
                *(state.offset(LOCAL_APIC_OFFSET_ISR + i * 16) as *mut u32) =
                    hv_state.apic_isr[i as usize];
                *(state.offset(LOCAL_APIC_OFFSET_TMR + i * 16) as *mut u32) =
                    hv_state.apic_tmr[i as usize];
                *(state.offset(LOCAL_APIC_OFFSET_IRR + i * 16) as *mut u32) =
                    hv_state.apic_irr[i as usize];
            }
        }

        // Highest priority interrupt (isr = in service register) this is how WHP computes it
        let mut isrv: u32 = 0;
        for i in (0..8).rev() {
            let val: u32 = hv_state.apic_isr[i as usize];
            if val != 0 {
                isrv = 31 - val.leading_zeros(); // index of most significant set bit
                isrv += i * 4 * 8; // i don't know
                break;
            }
        }

        // TODO This is meant to be max(tpr, isrv), but tpr is not populated!
        unsafe {
            *(state.offset(LOCAL_APIC_OFFSET_PPR) as *mut u32) = isrv;
        }
        ret
    }
}

impl From<LapicState> for mshv_vp_state {
    fn from(reg: LapicState) -> Self {
        let state = reg.regs.as_ptr();
        let mut vp_state: mshv_vp_state = mshv_vp_state::default();
        unsafe {
            let mut lapic_state = hv_local_interrupt_controller_state {
                apic_id: *(state.offset(LOCAL_APIC_OFFSET_APIC_ID) as *mut u32),
                apic_version: *(state.offset(LOCAL_APIC_OFFSET_VERSION) as *mut u32),
                apic_remote_read: *(state.offset(LOCAL_APIC_OFFSET_REMOTE_READ) as *mut u32),
                apic_ldr: *(state.offset(LOCAL_APIC_OFFSET_LDR) as *mut u32),
                apic_dfr: *(state.offset(LOCAL_APIC_OFFSET_DFR) as *mut u32),
                apic_spurious: *(state.offset(LOCAL_APIC_OFFSET_SPURIOUS) as *mut u32),
                apic_esr: *(state.offset(LOCAL_APIC_OFFSET_ERROR) as *mut u32),
                apic_icr_low: *(state.offset(LOCAL_APIC_OFFSET_ICR_LOW) as *mut u32),
                apic_icr_high: *(state.offset(LOCAL_APIC_OFFSET_ICR_HIGH) as *mut u32),
                apic_lvt_timer: *(state.offset(LOCAL_APIC_OFFSET_TIMER_LVT) as *mut u32),
                apic_lvt_thermal: *(state.offset(LOCAL_APIC_OFFSET_THERMAL_LVT) as *mut u32),
                apic_lvt_perfmon: *(state.offset(LOCAL_APIC_OFFSET_PERFMON_LVT) as *mut u32),
                apic_lvt_lint0: *(state.offset(LOCAL_APIC_OFFSET_LINT0_LVT) as *mut u32),
                apic_lvt_lint1: *(state.offset(LOCAL_APIC_OFFSET_LINT1_LVT) as *mut u32),
                apic_lvt_error: *(state.offset(LOCAL_APIC_OFFSET_ERROR_LVT) as *mut u32),
                apic_initial_count: *(state.offset(LOCAL_APIC_OFFSET_INITIAL_COUNT) as *mut u32),
                apic_counter_value: *(state.offset(LOCAL_APIC_OFFSET_CURRENT_COUNT) as *mut u32),
                apic_divide_configuration: *(state.offset(LOCAL_APIC_OFFSET_DIVIDER) as *mut u32),
                apic_error_status: 0,
                apic_lvt_cmci: 0,
                apic_isr: [0; 8],
                apic_tmr: [0; 8],
                apic_irr: [0; 8],
            };

            /* vectors ISR TMR IRR */
            for i in 0..8 {
                lapic_state.apic_isr[i as usize] =
                    *(state.offset(LOCAL_APIC_OFFSET_ISR + i * 16) as *mut u32);
                lapic_state.apic_tmr[i as usize] =
                    *(state.offset(LOCAL_APIC_OFFSET_TMR + i * 16) as *mut u32);
                lapic_state.apic_irr[i as usize] =
                    *(state.offset(LOCAL_APIC_OFFSET_IRR + i * 16) as *mut u32);
            }
            vp_state.type_ =
                hv_get_set_vp_state_type_HV_GET_SET_VP_STATE_LOCAL_INTERRUPT_CONTROLLER_STATE;
            vp_state.buf_size = 1024;
            let boxed_obj = Box::new(lapic_state);
            vp_state.buf.lapic = Box::into_raw(boxed_obj);
        }
        vp_state
    }
}
// implement `Display` for `XSave`
impl fmt::Display for XSave {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "flags: {}, states: {}, buffer_size: {}, buffer: {:?}\n data: {:02X?}",
            self.flags(),
            self.states(),
            self.data_size(),
            self.data_buffer(),
            self.buffer,
        )
    }
}
// Implement XSave to retrieve each field from the buffer
impl XSave {
    pub fn flags(&self) -> u64 {
        let array: [u8; 8] = [0, 0, 0, 0, 0, 0, 0, 0];
        unsafe {
            ptr::copy(
                self.buffer.as_ptr().offset(0) as *mut u8,
                array.as_ptr() as *mut u8,
                8,
            )
        };
        u64::from_le_bytes(array)
    }
    pub fn states(&self) -> u64 {
        let array: [u8; 8] = [0, 0, 0, 0, 0, 0, 0, 0];
        unsafe {
            ptr::copy(
                self.buffer.as_ptr().offset(8) as *mut u8,
                array.as_ptr() as *mut u8,
                8,
            )
        };
        u64::from_le_bytes(array)
    }
    pub fn data_size(&self) -> u64 {
        let array: [u8; 8] = [0, 0, 0, 0, 0, 0, 0, 0];
        unsafe {
            ptr::copy(
                self.buffer.as_ptr().offset(16) as *mut u8,
                array.as_ptr() as *mut u8,
                8,
            )
        };
        u64::from_le_bytes(array)
    }
    pub fn data_buffer(&self) -> *const u8 {
        unsafe { self.buffer.as_ptr().offset(24) as *mut u8 }
    }
}

#[repr(C)]
#[derive(Debug, Default, Copy, Clone, Eq, PartialEq, AsBytes, FromBytes)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct SuspendRegisters {
    pub explicit_register: u64,
    pub intercept_register: u64,
}
#[repr(C)]
#[derive(Debug, Default, Copy, Clone, Eq, PartialEq, AsBytes, FromBytes)]
#[cfg_attr(feature = "with-serde", derive(Deserialize, Serialize))]
pub struct MiscRegs {
    pub hypercall: u64,
}
